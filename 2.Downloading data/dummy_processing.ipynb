{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pre-process the files\n",
    "def process_file(file_path):    \n",
    "    # df = pd.read_csv(\"aemo_data/High_Impact_Outages_20210830.csv\", encoding='latin1')\n",
    "    df = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "    # certain columns have \\n and \\r, so they are removed\n",
    "    df = df.replace({r\"\\r|\\n\": \" \"}, regex=True)\n",
    "    df.columns = df.columns.str.replace(r\"[\\r\\n]\", \" \", regex=True)\n",
    "\n",
    "    # drop the columns which are Unnamed and have no values (total of 7 columns)\n",
    "    # df = df.iloc[:, :-7]\n",
    "\n",
    "    # pre-processing on the columns\n",
    "    # extract information from the 'start' and 'finish' columns and separate into separate columns\n",
    "    pattern = r\"(\\d{2}/\\d{2}/\\d{4})\\s+(\\d{2}:\\d{2})\\s+(\\w+)\"\n",
    "    df[[\"Start_Date\", \"Start_Time\", \"Start_Day\"]] = df[\"Start\"].str.extract(pattern)\n",
    "    df[[\"Finish_Date\", \"Finish_Time\", \"Finish_Day\"]] = df[\"Finish\"].str.extract(pattern)\n",
    "    df['Start_Date'] = pd.to_datetime(df['Start_Date'], dayfirst=True)\n",
    "    df['Start_Time'] = pd.to_datetime(df['Start_Time'], format='%H:%M').dt.strftime('%H:%M:%S')\n",
    "    df['Finish_Date'] = pd.to_datetime(df['Finish_Date'], dayfirst=True)\n",
    "    df['Finish_Time'] = pd.to_datetime(df['Finish_Time'], format='%H:%M').dt.strftime('%H:%M:%S')\n",
    "\n",
    "    # extract information from 'recall'\n",
    "    pattern = r\"Day:\\s*(\\d+)\\s*hr?s?(?:-Night:\\s*(\\d+)\\s*hr?s?)?\"\n",
    "    df[[\"Recall_Day_Hours\", \"Recall_Night_Hours\"]] = df[\"Recall\"].str.extract(pattern)\n",
    "    df[\"Recall_Day_Hours\"] = pd.to_numeric(df[\"Recall_Day_Hours\"], errors=\"coerce\")\n",
    "    df['Recall_Day_Hours'] = df['Recall_Day_Hours'].fillna(0)\n",
    "    df[\"Recall_Night_Hours\"] = pd.to_numeric(df[\"Recall_Night_Hours\"], errors=\"coerce\")\n",
    "    df['Recall_Night_Hours'] = df['Recall_Night_Hours'].fillna(0)\n",
    "\n",
    "    # extract information from 'reason'\n",
    "    df[\"Reason\"] = df[\"Reason and  Duration\"].str.extract(r\"([a-zA-Z\\s]+)\")\n",
    "\n",
    "    # extract information from 'duration'\n",
    "    pattern = r\"([\\d\\.]+)\\s*(Days?|Hours?|Minutes?)\"\n",
    "    df[[\"Value\", \"Unit\"]] = df[\"Duration\"].str.extract(pattern)\n",
    "    df[\"Value\"] = df[\"Value\"].astype(float)\n",
    "    df[\"Duration_Hours\"] = df.apply(\n",
    "        lambda row: row[\"Value\"] * 24 if pd.notnull(row[\"Unit\"]) and \"Day\" in row[\"Unit\"]  # Convert Days to Hours\n",
    "        else row[\"Value\"] if pd.notnull(row[\"Unit\"]) and \"Hour\" in row[\"Unit\"]  # Keep Hours as is\n",
    "        else row[\"Value\"] / 60 if pd.notnull(row[\"Unit\"]) and \"Minute\" in row[\"Unit\"]  # Convert Minutes to Hours\n",
    "        else None, axis=1  # Handle any unexpected cases\n",
    "    )\n",
    "\n",
    "    # split information from 'status'\n",
    "    df[[\"Status_Description\", \"Status_Code\"]] = df[\"Status\"].str.split(\" - \", expand=True)\n",
    "    # Split information from 'status and market notice'\n",
    "    df[['Status_Description_Market', 'Market_Notice_Code']] = df['Status and  Market Notice'].str.split(\" - \", expand=True)\n",
    "\n",
    "    # List of boolean columns\n",
    "    bool_cols = ['Project Work?', 'Unplanned?', 'Generator Aware?', 'DNSP Aware?', 'Inter-Regional']\n",
    "    # Replace 'T' with 1 and NaN with 0\n",
    "    df[bool_cols] = df[bool_cols].apply(lambda col: col.map(lambda x: 1 if x == 'T' else 0))\n",
    "\n",
    "    # drop non-needed columns now:\n",
    "    df.drop(columns=['Start', 'Finish', 'Status', 'Status and  Market Notice', 'Duration', 'Value', 'Unit',\n",
    "                    'Reason and  Duration', 'Recall', 'Start', 'Finish'], inplace=True)\n",
    "\n",
    "    # re-order columns\n",
    "    cols = list(df.columns)\n",
    "    new_col_order = ['Region', 'NSP', \n",
    "                    'Start_Date', 'Start_Time', 'Start_Day', # Start time information\n",
    "                    'Finish_Date', 'Finish_Time', 'Finish_Day', # End time information\n",
    "                    'Network Asset', # identifying information\n",
    "                    'Recall_Day_Hours', 'Recall_Night_Hours', # recall information\n",
    "                    'Project Work?', 'Unplanned?', 'DNSP Aware?', 'Generator Aware?', 'Inter-Regional', # boolean terms\n",
    "                    'Status_Description', 'Status_Code', 'Status_Description_Market', 'Market_Notice_Code', # status information\n",
    "                    'Reason', 'Duration_Hours', # reason and duration information\n",
    "                    'Impact'\n",
    "                    ]\n",
    "    new_col_order = list(dict.fromkeys(new_col_order))\n",
    "    df = df[new_col_order]\n",
    "\n",
    "    # remove rows where 'Region' is empty\n",
    "    df = df[df['Region'].notna() & (df['Region'].str.strip() != '')]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 92 entries, 0 to 91\n",
      "Data columns (total 23 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   Region                     92 non-null     object        \n",
      " 1   NSP                        92 non-null     object        \n",
      " 2   Start_Date                 92 non-null     datetime64[ns]\n",
      " 3   Start_Time                 92 non-null     object        \n",
      " 4   Start_Day                  92 non-null     object        \n",
      " 5   Finish_Date                92 non-null     datetime64[ns]\n",
      " 6   Finish_Time                92 non-null     object        \n",
      " 7   Finish_Day                 92 non-null     object        \n",
      " 8   Network Asset              92 non-null     object        \n",
      " 9   Recall_Day_Hours           92 non-null     float64       \n",
      " 10  Recall_Night_Hours         92 non-null     float64       \n",
      " 11  Project Work?              92 non-null     int64         \n",
      " 12  Unplanned?                 92 non-null     int64         \n",
      " 13  DNSP Aware?                92 non-null     int64         \n",
      " 14  Generator Aware?           92 non-null     int64         \n",
      " 15  Inter-Regional             92 non-null     int64         \n",
      " 16  Status_Description         92 non-null     object        \n",
      " 17  Status_Code                81 non-null     object        \n",
      " 18  Status_Description_Market  92 non-null     object        \n",
      " 19  Market_Notice_Code         81 non-null     object        \n",
      " 20  Reason                     92 non-null     object        \n",
      " 21  Duration_Hours             92 non-null     float64       \n",
      " 22  Impact                     92 non-null     object        \n",
      "dtypes: datetime64[ns](2), float64(3), int64(5), object(13)\n",
      "memory usage: 17.2+ KB\n"
     ]
    }
   ],
   "source": [
    "process_file(\"aemo_data/High_Impact_Outages_20210830.csv\")\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Region'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Loop through each file, apply the processing function and collect the results\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[1;32m----> 9\u001b[0m     processed_df \u001b[38;5;241m=\u001b[39m process_file(file)\n\u001b[0;32m     10\u001b[0m     processed_dfs\u001b[38;5;241m.\u001b[39mappend(processed_df)\n",
      "Cell \u001b[1;32mIn[27], line 72\u001b[0m, in \u001b[0;36mprocess_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     61\u001b[0m new_col_order \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNSP\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     62\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart_Date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart_Time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart_Day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# Start time information\u001b[39;00m\n\u001b[0;32m     63\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish_Date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish_Time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish_Day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# End time information\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImpact\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     70\u001b[0m                 ]\n\u001b[0;32m     71\u001b[0m new_col_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(new_col_order))\n\u001b[1;32m---> 72\u001b[0m df \u001b[38;5;241m=\u001b[39m df[new_col_order]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove rows where 'Region' is empty\u001b[39;00m\n\u001b[0;32m     75\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna() \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\althi\\anaconda3_2\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\althi\\anaconda3_2\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\althi\\anaconda3_2\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Region'] not in index\""
     ]
    }
   ],
   "source": [
    "# List of CSV files to process\n",
    "csv_files = glob.glob(\"aemo_data/*.csv\")\n",
    "\n",
    "# List to store processed dataframes\n",
    "processed_dfs = []\n",
    "\n",
    "# Loop through each file, apply the processing function and collect the results\n",
    "for file in csv_files:\n",
    "    processed_df = process_file(file)\n",
    "    processed_dfs.append(processed_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
