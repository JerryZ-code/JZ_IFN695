{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import os, glob, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pre-process the files\n",
    "def process_file(file_path):    \n",
    "    df = pd.read_csv(\"aemo_data/High_Impact_Outages_20210830.csv\", encoding='latin1')\n",
    "    # df = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "    # add a column to keep track of the csv file in which each entry came from\n",
    "    base_name = os.path.basename(file_path)\n",
    "    date_match = re.search(r'(\\d{8})', base_name)\n",
    "    if date_match:\n",
    "        file_date = pd.to_datetime(date_match.group(1), format='%Y%m%d').date()\n",
    "    else:\n",
    "        file_date = None  # fallback if no date in filename\n",
    "    df[\"file_name\"] = file_date\n",
    "\n",
    "    # drop the 'Vs Old' column, it has no meaning\n",
    "    if 'Vs Old' in df.columns:\n",
    "            df.drop(columns=['Vs Old'], inplace=True)\n",
    "\n",
    "    # certain columns have \\n and \\r, so they are removed\n",
    "    df = df.replace({r\"\\r|\\n\": \" \"}, regex=True)\n",
    "    df.columns = df.columns.str.replace(r\"[\\r\\n]\", \" \", regex=True)\n",
    "\n",
    "    # drop the columns which are Unnamed and have no values (total of 7 columns)\n",
    "    # df = df.iloc[:, :-7]\n",
    "\n",
    "    # pre-processing on the columns\n",
    "    # extract information from the 'start' and 'finish' columns and separate into separate columns\n",
    "    df['Start'] = df['Start'].replace({r'\\r|\\n': ' '}, regex=True)\n",
    "    df['Finish'] = df['Finish'].replace({r'\\r|\\n': ' '}, regex=True)\n",
    "    df['Start'] = pd.to_datetime(df['Start'], dayfirst=True, format='%d/%m/%Y %H:%M %A', errors='coerce')\n",
    "    df['Finish'] = pd.to_datetime(df['Finish'], dayfirst=True, format='%d/%m/%Y %H:%M %A', errors='coerce')\n",
    "\n",
    "    # extract information from 'recall'\n",
    "    pattern = r\"Day:\\s*(\\d+)\\s*hr?s?(?:-Night:\\s*(\\d+)\\s*hr?s?)?\"\n",
    "    df[[\"Recall_Day_Hours\", \"Recall_Night_Hours\"]] = df[\"Recall\"].str.extract(pattern)\n",
    "    df[\"Recall_Day_Hours\"] = pd.to_numeric(df[\"Recall_Day_Hours\"], errors=\"coerce\")\n",
    "    df['Recall_Day_Hours'] = df['Recall_Day_Hours'].fillna(0)\n",
    "    df[\"Recall_Night_Hours\"] = pd.to_numeric(df[\"Recall_Night_Hours\"], errors=\"coerce\")\n",
    "    df['Recall_Night_Hours'] = df['Recall_Night_Hours'].fillna(0)\n",
    "\n",
    "    # extract information for 'Impact region, reason and duration'\n",
    "    # this column only appears as of 28/03/2022\n",
    "    if \"Impact Region, Reason and Duration\" in df.columns:\n",
    "        impact_reason_pattern = r\"^([\\w\\s/]+?)\\s+([a-zA-Z\\s]+?)\\s+([\\d.]+)\\s*(Days?|Hours?|Minutes?)$\"\n",
    "        temp_cols = df[\"Impact Region, Reason and Duration\"].str.extract(impact_reason_pattern)\n",
    "        temp_cols.columns = ['Impact_Region', 'Reason', 'Value', 'Unit']\n",
    "        temp_cols[\"Value\"] = temp_cols[\"Value\"].astype(float)\n",
    "        df[\"Impact_Region\"] = temp_cols[\"Impact_Region\"].str.strip()\n",
    "        df[\"Reason\"] = temp_cols[\"Reason\"].str.strip()\n",
    "        df[\"Duration_Hours\"] = temp_cols.apply(\n",
    "            lambda row: row[\"Value\"] * 24 if pd.notnull(row[\"Unit\"]) and \"Day\" in row[\"Unit\"]\n",
    "            else row[\"Value\"] if pd.notnull(row[\"Unit\"]) and \"Hour\" in row[\"Unit\"]\n",
    "            else row[\"Value\"] / 60 if pd.notnull(row[\"Unit\"]) and \"Minute\" in row[\"Unit\"]\n",
    "            else None, axis=1)\n",
    "    else:\n",
    "        # if the column does not exist, then make let the impact region and region be the same\n",
    "        df[\"Reason\"] = df[\"Reason and  Duration\"].str.extract(r\"([a-zA-Z\\s]+)\")\n",
    "        duration_pattern = r\"([\\d\\.]+)\\s*(Days?|Hours?|Minutes?)\"\n",
    "        df[[\"Value\", \"Unit\"]] = df[\"Duration\"].str.extract(duration_pattern)\n",
    "        df[\"Value\"] = df[\"Value\"].astype(float)\n",
    "        df[\"Duration_Hours\"] = df.apply(\n",
    "            lambda row: row[\"Value\"] * 24 if pd.notnull(row[\"Unit\"]) and \"Day\" in row[\"Unit\"]\n",
    "            else row[\"Value\"] if pd.notnull(row[\"Unit\"]) and \"Hour\" in row[\"Unit\"]\n",
    "            else row[\"Value\"] / 60 if pd.notnull(row[\"Unit\"]) and \"Minute\" in row[\"Unit\"]\n",
    "            else None, axis=1)\n",
    "        df[\"Impact_Region\"] = df[\"Region\"]\n",
    "\n",
    "    # split information from 'status'\n",
    "    df[[\"Status_Description\", \"Status_Code\"]] = df[\"Status\"].str.split(\" - \", expand=True)\n",
    "    # Split information from 'status and market notice'\n",
    "    df[['Status_Description_Market', 'Market_Notice_Code']] = df['Status and  Market Notice'].str.split(\" - \", expand=True)\n",
    "\n",
    "    # List of boolean columns\n",
    "    bool_cols = ['Project Work?', 'Unplanned?', 'Generator Aware?', 'DNSP Aware?', 'Inter-Regional']\n",
    "    # Replace 'T' with 1 and NaN with 0\n",
    "    df[bool_cols] = df[bool_cols].apply(lambda col: col.map(lambda x: 1 if x == 'T' else 0))\n",
    "\n",
    "    # drop non-needed columns now:\n",
    "    df.drop(columns=['Status', 'Status and  Market Notice', 'Duration', 'Value', 'Unit',\n",
    "                    'Reason and  Duration', 'Recall'], inplace=True)\n",
    "\n",
    "    # re-order columns\n",
    "    cols = list(df.columns)\n",
    "    new_col_order = ['Region', 'NSP', \n",
    "                    'Start', 'Finish', # Start and Finish information\n",
    "                    'Network Asset', # identifying information\n",
    "                    'Recall_Day_Hours', 'Recall_Night_Hours', # recall information\n",
    "                    'Project Work?', 'Unplanned?', 'DNSP Aware?', 'Generator Aware?', 'Inter-Regional', # boolean terms\n",
    "                    'Status_Description', 'Status_Code', 'Status_Description_Market', 'Market_Notice_Code', # status information\n",
    "                    'Impact_Region', 'Reason', 'Duration_Hours', # reason and duration information\n",
    "                    'Impact', 'file_name'\n",
    "                    ]\n",
    "    new_col_order = list(dict.fromkeys(new_col_order))\n",
    "    df = df[new_col_order]\n",
    "\n",
    "    # remove rows where 'Region' is empty\n",
    "    df = df[df['Region'].notna() & (df['Region'].str.strip() != '')]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV files to process\n",
    "csv_files = glob.glob(\"aemo_data/*.csv\")\n",
    "\n",
    "# List to store processed dataframes\n",
    "processed_dfs = []\n",
    "\n",
    "# Loop through each file, apply the processing function and collect the results\n",
    "for file in csv_files:\n",
    "    processed_df = process_file(file)\n",
    "    processed_dfs.append(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13156 entries, 0 to 13155\n",
      "Data columns (total 21 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   Region                     13156 non-null  object        \n",
      " 1   NSP                        13156 non-null  object        \n",
      " 2   Start                      13156 non-null  datetime64[ns]\n",
      " 3   Finish                     13156 non-null  datetime64[ns]\n",
      " 4   Network Asset              13156 non-null  object        \n",
      " 5   Recall_Day_Hours           13156 non-null  float64       \n",
      " 6   Recall_Night_Hours         13156 non-null  float64       \n",
      " 7   Project Work?              13156 non-null  int64         \n",
      " 8   Unplanned?                 13156 non-null  int64         \n",
      " 9   DNSP Aware?                13156 non-null  int64         \n",
      " 10  Generator Aware?           13156 non-null  int64         \n",
      " 11  Inter-Regional             13156 non-null  int64         \n",
      " 12  Status_Description         13156 non-null  object        \n",
      " 13  Status_Code                11583 non-null  object        \n",
      " 14  Status_Description_Market  13156 non-null  object        \n",
      " 15  Market_Notice_Code         11583 non-null  object        \n",
      " 16  Impact_Region              13156 non-null  object        \n",
      " 17  Reason                     13156 non-null  object        \n",
      " 18  Duration_Hours             13156 non-null  float64       \n",
      " 19  Impact                     13156 non-null  object        \n",
      " 20  file_name                  13156 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(3), int64(5), object(11)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# concatenate all dataframes and export to csv\n",
    "full_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "full_df.info()\n",
    "full_df.to_csv(\"processed_high_impact_outages.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
